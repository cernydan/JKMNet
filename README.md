# ğŸ§  JKMNet â€” MLP Ensemble & Prediction Framework

`JKMNet` is a C++20 project for training and applying **feed-forward neural networks (MLP)** to environmental and time-series data.  
The framework supports **parallel ensemble training**, **Adam optimizer**, and **post-hoc prediction from saved weights**.

---

## âœ¨ Main Features
- Multi-layer perceptron (MLP) implementation with customizable architecture  
- Parallel ensemble training using OpenMP  
- Multiple training modes: online, batch, epoch-based  
- Transformations of inputs/outputs (e.g. MinMax, Z-score)  
- Saving and reloading of model weights for later predictions  
- CSV-based input/output for easy integration with other tools

---

## ğŸ“¦ Requirements
- Linux or macOS (tested on Ubuntu 22.04)  
- `g++` with C++20 support  
- [OpenMP](https://www.openmp.org/) for parallel processing  
- [Eigen](https://eigen.tuxfamily.org/) for linear algebra (header-only library)  
- `make`

---

## ğŸ§° Installation

### 1. Clone the repository
```bash
git clone https://github.com/<your-username>/JKMNet.git
cd JKMNet
```

### 2. Compile the project
```bash
make
```
This will:
- create the `obj` and `bin` directories
- build the executable at `bin/JKMNet`

If you want to clean the build:
```bash
make clean
```

---

## ğŸ§ª Running the program

### ğŸ”¹ 1. Training + Testing Mode

To train an ensemble of MLPs and test it on other data:

```bash
./bin/JKMNet [threads]
```

Example:
```bash
./bin/JKMNet 4
```
This will train using 4 threads and save:
- trained weights  
- calibration and validation predictions  
- metrics per run  
- log files in `data/outputs/`

---

### ğŸ”¹ 2. Prediction Mode

Once the model has been trained and weights are saved, you can run the prediction mode:

```bash
./bin/JKMNet predict
```
This will use the **default weights file** path specified in `settings/config_model.ini`.

You can also specify a custom weights file:

```bash
./bin/JKMNet predict data/outputs/weights/weights_final_1.csv
```

Optionally, you can specify the number of threads:
```bash
./bin/JKMNet predict data/outputs/weights/weights_final_1.csv 4
```

If the weight file doesnâ€™t exist, the program will exit cleanly with a clear error message.

---

## âš™ï¸ Configuration

All model and training settings are specified in:
```
settings/config_model.ini
```

Example parameters:
```ini
[data]
data_file = data/inputs/input_data.csv
columns = T1, T2, T3, moisture

[model]
architecture = 8, 6, 4
activation = RELU
trainer = online

[training]
ensemble_runs = 20
max_iterations = 500
learning_rate = 0.001
train_fraction = 0.8

[paths]
weights_csv = data/outputs/weights/weights_final.csv
```

---

## ğŸ§  Example Workflow

1. Prepare your dataset as a CSV file.  
2. Configure the model architecture and paths in `config_model.ini`.  
3. Run training and testing:
   ```bash
   ./bin/JKMNet 4
   ```
4. Use the trained weights for prediction:
   ```bash
   ./bin/JKMNet predict
   ```
5. Check outputs in `data/outputs/`:
   - `weights/` â€” saved model weights  
   - `metrics/` â€” calibration and validation metrics  
   - `logs/` â€” per-run log files  
   - `*.csv` â€” predicted values

---

## ğŸ§­ Project Structure

```
JKMNet/
â”œâ”€â”€ src/                    # C++ source files
â”œâ”€â”€ include/                # Header files
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ inputs/             # Input data
â”‚   â””â”€â”€ outputs/            # Outputs, logs, metrics
â”œâ”€â”€ settings/               # Configuration files
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â””â”€â”€ bin/                    # Compiled binary
```

---

## ğŸ§‘â€ğŸ’» Development Notes
- The modular structure allows easy extension with new optimizers or layer types (e.g., CNN or hybrid models).  
- Parallelization is handled via OpenMP (`#pragma omp parallel for`) for ensemble training.

---

## ğŸ“œ License
MIT License.  

---
